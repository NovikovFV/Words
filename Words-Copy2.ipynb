{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "DICT = []\n",
    "\n",
    "STATE_WORD = 0\n",
    "STATE_TYPE = 1\n",
    "STATE_MEANING_START = 2\n",
    "\n",
    "class DictProcessor():\n",
    "    def __init__(self):\n",
    "        self.word = \"\"\n",
    "        self.meaning = \"\"\n",
    "        \n",
    "        self.state = STATE_WORD\n",
    "        \n",
    "        self.dict = []\n",
    "\n",
    "    def update(self, line):\n",
    "        if line == \"\":\n",
    "            self.go(STATE_WORD)\n",
    "            return    \n",
    "        elif re.match(r\"^\\s*\\d+\\.(.*)\", line):\n",
    "            m = re.match(r\"^\\s*\\d+\\.(.*)\", line)\n",
    "            line = m.group(1)\n",
    "            self.go(STATE_TYPE)\n",
    "        elif re.match(r\"^\\s*\\d+\\)\", line):   \n",
    "            m = re.match(r\"^\\s*\\d+\\)(.*)\", line)\n",
    "            line = m.group(1)\n",
    "            self.go(STATE_MEANING_START)\n",
    "        \n",
    "        self._process(line)\n",
    "        \n",
    "    def _process(self, line):\n",
    "        if self.state == STATE_WORD:\n",
    "            self.word = line\n",
    "            self.go(STATE_TYPE)\n",
    "        elif self.state == STATE_TYPE:\n",
    "            pass\n",
    "            self.go(STATE_MEANING_START)\n",
    "        elif self.state == STATE_MEANING_START:\n",
    "            self.meaning += \" \" + line.strip()\n",
    "    \n",
    "    def _publish(self):\n",
    "        self.meaning = self.meaning.strip()\n",
    "        if self.word != \"\" and self.meaning != \"\" and len(self.word) >= 3:\n",
    "            self.dict.append((self.word, self.meaning))\n",
    "            \n",
    "            #print(\"!!!| [%s] - [%s]\" % (self.word, self.meaning))        \n",
    "        self.meaning = \"\"                        \n",
    "    \n",
    "    def go(self, new_state):\n",
    "        if self.state == STATE_MEANING_START:\n",
    "            self._publish()\n",
    "        \n",
    "        self.state = new_state        \n",
    "\n",
    "processor = DictProcessor()        \n",
    "        \n",
    "with open(\"efremova.txt\") as fin:\n",
    "    row = -1\n",
    "    \n",
    "    state = STATE_WORD\n",
    "    \n",
    "    word = \"\"\n",
    "    for line in fin:\n",
    "        row += 1\n",
    "        \n",
    "        if row < 3:\n",
    "            continue\n",
    "            \n",
    "        if row > 100 and False:\n",
    "            break\n",
    "        \n",
    "        line = line.strip()                \n",
    "        \n",
    "        #print(\"%d: %s\" % (processor.state, line))\n",
    "        \n",
    "        processor.update(line)                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200438\n"
     ]
    }
   ],
   "source": [
    "print(len(processor.dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_tr = TfidfVectorizer(analyzer='char', ngram_range=(2,4))\n",
    "f_words = words_tr.fit_transform((w_m[0] for w_m in processor.dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "cache = {}\n",
    "\n",
    "def preprocess(word):\n",
    "    if word not in cache:\n",
    "        cache[word] = morph.parse(word)[0].normal_form\n",
    "        \n",
    "    return cache[word]    \n",
    "\n",
    "def tokenizer(line):\n",
    "    res = []\n",
    "    for w in re.findall(\"\\w+\", line):\n",
    "        res.append(preprocess(w))\n",
    "        \n",
    "    return res    \n",
    "\n",
    "meaning_tr = TfidfVectorizer(tokenizer=tokenizer)\n",
    "f_meaning = meaning_tr.fit_transform((w_m[1] for w_m in processor.dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "а-а\n",
      "также-также\n",
      "я-я\n",
      "ть-ть\n",
      "словообразовательная-словообразовательный\n",
      "единица-единица\n",
      "образующая-образующая\n",
      "как-как\n",
      "переходные-переходный\n",
      "так-так\n",
      "и-и\n",
      "непереходные-непереходный\n",
      "глаголы-глагол\n",
      "несовершенного-несовершенный\n",
      "вида-вид\n",
      "с-с\n",
      "общим-общий\n",
      "значением-значение\n",
      "действия-действие\n",
      "которое-который\n",
      "имеет-иметь\n",
      "отношение-отношение\n"
     ]
    }
   ],
   "source": [
    "for i, (k, v) in enumerate(cache.items()):\n",
    "    print(\"%s-%s\" % (k, v))\n",
    "    if i > 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "1900\n",
      "1903\n",
      "1904\n",
      "1905\n",
      "1906\n",
      "1907\n",
      "1908\n",
      "1910\n",
      "1912\n",
      "1914\n",
      "1917\n",
      "1918\n",
      "1919\n",
      "1920\n",
      "1921\n",
      "1922\n",
      "1923\n",
      "1926\n",
      "1927\n",
      "1928\n",
      "1929\n",
      "1930\n",
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "1946\n",
      "1947\n",
      "1948\n",
      "1960\n",
      "1966\n",
      "1968\n",
      "1971\n",
      "1972\n",
      "1979\n",
      "1980\n",
      "1989\n",
      "1991\n",
      "1993\n",
      "1а1\n",
      "1а2\n",
      "1а3\n",
      "1а4\n",
      "1а5\n",
      "1а6\n",
      "1а7\n",
      "1а8\n",
      "2\n",
      "20\n",
      "200\n",
      "206\n",
      "21\n",
      "210\n",
      "212\n",
      "22\n",
      "23\n",
      "239\n",
      "24\n",
      "2400\n",
      "244\n",
      "25\n",
      "2500\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "2а1\n",
      "2а2\n",
      "2а3\n",
      "2а4\n",
      "2а5\n",
      "3\n",
      "30\n",
      "300\n",
      "3000\n",
      "3048\n",
      "32\n",
      "330\n",
      "335\n",
      "34\n",
      "35\n",
      "356\n",
      "36\n",
      "360\n",
      "3600\n",
      "365\n",
      "366\n",
      "3а1\n",
      "3а2\n",
      "3а3\n",
      "4\n",
      "40\n",
      "400\n",
      "4047\n",
      "409\n",
      "44\n",
      "45\n",
      "450\n",
      "453\n",
      "46\n",
      "480\n",
      "489\n",
      "491\n",
      "4а2\n",
      "5\n",
      "50\n",
      "500\n",
      "52\n",
      "54\n",
      "58\n",
      "5а1\n",
      "5а2\n",
      "6\n",
      "60\n",
      "600\n",
      "6000\n",
      "64\n",
      "66\n",
      "7\n",
      "70\n",
      "700\n",
      "711\n",
      "72\n",
      "750\n",
      "772\n",
      "775\n",
      "79\n",
      "8\n",
      "80\n",
      "800\n",
      "81\n",
      "83\n",
      "850\n",
      "86\n",
      "87\n",
      "88\n",
      "9\n",
      "90\n",
      "900\n",
      "91\n",
      "96\n",
      "962\n",
      "b\n",
      "c\n",
      "color\n",
      "font\n",
      "i\n",
      "ii\n",
      "iii\n",
      "iv\n",
      "ix\n",
      "j\n",
      "p\n",
      "pluralia\n",
      "r\n",
      "red\n",
      "s\n",
      "tantum\n",
      "v\n",
      "vi\n",
      "vii\n",
      "viii\n",
      "x\n",
      "x1x\n",
      "xi\n",
      "xii\n",
      "xiii\n",
      "xiv\n",
      "xix\n",
      "xv\n",
      "xvi\n",
      "xvii\n",
      "xviii\n",
      "xx\n",
      "а\n",
      "абажур\n",
      "абазин\n",
      "абазинец\n",
      "абазинский\n",
      "абак\n",
      "аббат\n",
      "аббатиса\n",
      "аббатисса\n",
      "аббатство\n",
      "аббревиатура\n",
      "абвер\n",
      "аберрация\n",
      "абзац\n",
      "абиогенез\n",
      "абиогенный\n",
      "абиссаль\n"
     ]
    }
   ],
   "source": [
    "for i, w in enumerate(meaning_tr.get_feature_names()):\n",
    "    if i > 100:\n",
    "        print(w)\n",
    "    if i > 300:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200438, 61895)\n"
     ]
    }
   ],
   "source": [
    "print(f_meaning.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 50215)\t0.139517528718\n",
      "  (0, 11974)\t0.126804085358\n",
      "  (0, 29542)\t0.148867033309\n",
      "  (0, 17059)\t0.104321051909\n",
      "  (0, 53371)\t0.160321103939\n",
      "  (0, 15264)\t0.0721948287248\n",
      "  (0, 27242)\t0.218367549496\n",
      "  (0, 8437)\t0.134149793405\n",
      "  (0, 27752)\t0.203546461353\n",
      "  (0, 4678)\t0.114313103308\n",
      "  (0, 47718)\t0.065060408253\n",
      "  (0, 15102)\t0.127403068793\n",
      "  (0, 54392)\t0.198647783618\n",
      "  (0, 59716)\t0.133466553303\n",
      "  (0, 24702)\t0.126598343328\n",
      "  (0, 23722)\t0.283250347002\n",
      "  (0, 8)\t0.070986946694\n",
      "  (0, 158)\t0.081976616484\n",
      "  (0, 3739)\t0.0627078379595\n",
      "  (0, 15948)\t0.164774034023\n",
      "  (0, 54081)\t0.167292231629\n",
      "  (0, 4329)\t0.15990045476\n",
      "  (0, 48739)\t0.114948185391\n",
      "  (0, 16645)\t0.301636281446\n",
      "  (0, 12228)\t0.0864134946292\n",
      "  (0, 20100)\t0.088123690022\n",
      "  (0, 16045)\t0.265636023274\n",
      "  (0, 53067)\t0.286821266467\n",
      "  (0, 41202)\t0.162091685472\n",
      "  (0, 3265)\t0.221842579687\n",
      "  (0, 59764)\t0.244111269669\n",
      "  (0, 13293)\t0.211172995704\n",
      "  (0, 28487)\t0.24081517199\n"
     ]
    }
   ],
   "source": [
    "print(f_meaning[2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200438/200438 [02:26<00:00, 1367.82it/s]\n"
     ]
    }
   ],
   "source": [
    "BAD_W = 1\n",
    "\n",
    "from scipy.sparse import vstack, hstack\n",
    "from random import randint\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for i in tqdm(range(f_words.shape[0] // 1)):\n",
    "    f_w = f_words[i:i+1, :]\n",
    "    f_m = f_meaning[i:i+1, :]\n",
    "        \n",
    "    Y.append(1)\n",
    "    X.append(hstack([f_w, f_m]))\n",
    "    for j in range(BAD_W):        \n",
    "        rand_i = randint(0, f_words.shape[0] - 1)\n",
    "        if rand_i == i:\n",
    "            continue\n",
    "        Y.append(0)    \n",
    "        X.append(hstack([f_w, f_meaning[rand_i:rand_i+1, :]]))\n",
    "        \n",
    "    #break    \n",
    "X = vstack(X)    \n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400876, 127518)\n",
      "(400876,)\n",
      "(200438, 65623)\n",
      "(200438, 61895)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(f_words.shape)\n",
    "print(f_meaning.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = \"svm\"\n",
    "\n",
    "if t == \"svm\":\n",
    "    svm = LinearSVC()\n",
    "    clf = CalibratedClassifierCV(svm) \n",
    "    \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_proba = clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.57176645  0.42823355]\n",
      " [ 0.50361462  0.49638538]\n",
      " [ 0.4780626   0.5219374 ]\n",
      " ..., \n",
      " [ 0.52640057  0.47359943]\n",
      " [ 0.51461076  0.48538924]\n",
      " [ 0.49991339  0.50008661]]\n",
      "[1 0 1 ..., 1 0 0]\n",
      "0.734991726862\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(y_proba)\n",
    "print(y_test)\n",
    "print(roc_auc_score(y_test, y_proba[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.79975882  0.79924419  0.79792354  0.79371854  0.79101221  0.78797627\n",
      "  0.78739985  0.78705949  0.78550101  0.78548349]\n",
      "[ 0.20024118  0.79924419  0.20207646  0.20628146  0.20898779  0.21202373\n",
      "  0.21260015  0.21294051  0.78550101  0.78548349]\n",
      "[1 0 1 1 1 1 1 1 0 0]\n",
      "[13466  9853  9357  4438 11928  2836 10574 12862 10954   750]\n",
      "('ветровой', 'Возникающий в результате действия ветра.')\n",
      "('брать', 'разг. неперех. Захватывать приманку на рыболовном крючке, ловиться на удочку; клевать (о рыбе).')\n",
      "('боров', 'перен. разг.-сниж. Толстый, неповоротливый человек.')\n",
      "('бактерии', 'Одноклеточные микроорганизмы.')\n",
      "('василёк', 'Травянистое растение семейства сложноцветных с голубыми или синими цветками, растущее обычно во ржи и в посевах других злаков.')\n",
      "('апостольский', 'Соотносящийся по знач. с сущ.: апостол (2*), связанный с ним.')\n",
      "('буерный', 'Соотносящийся по знач. с сущ.: буер, связанный с ним.')\n",
      "('венчик', 'Часть цветка, состоящая из отдельных или сросшихся лепестков.')\n",
      "('бург', 'Укрепленный пункт, замок, вокруг которого в средние века в странах Западной Европы мог возникать город.')\n",
      "('аграрий', 'Тот, кто занимается земледелием, сельским хозяйством.')\n"
     ]
    }
   ],
   "source": [
    "#0.9 0.2 0.4 0.5\n",
    "#1 1 0 0\n",
    "\n",
    "diff = np.abs(y_proba[:, 1] - y_test)\n",
    "diff_order = np.argsort(diff)[::-1] #diff[diff_order] - decreasing\n",
    "\n",
    "N = 10\n",
    "\n",
    "#print(diff[diff_order])\n",
    "print(diff[diff_order[:N]])\n",
    "print(y_proba[:, 1][diff_order[:N]])\n",
    "print(y_test[diff_order[:N]])\n",
    "\n",
    "docs_nums = diff_order[:N] // (1 + BAD_W)\n",
    "print(docs_nums)\n",
    "\n",
    "for i in docs_nums:\n",
    "    print(processor.dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
